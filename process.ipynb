{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 19:01:33 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-01-10 19:01:33 INFO: Use device: gpu\n",
      "2023-01-10 19:01:33 INFO: Loading: tokenize\n",
      "2023-01-10 19:01:35 INFO: Loading: pos\n",
      "2023-01-10 19:01:37 INFO: Loading: lemma\n",
      "2023-01-10 19:01:37 INFO: Loading: depparse\n",
      "2023-01-10 19:01:38 INFO: Loading: sentiment\n",
      "2023-01-10 19:01:39 INFO: Loading: constituency\n",
      "2023-01-10 19:01:40 INFO: Loading: ner\n",
      "2023-01-10 19:01:41 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from nltk import Tree\n",
    "import stanza\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"air has higher resolution but the fonts are small .\"\n",
    "doc = nlp(text)\n",
    "sen = doc.sentences[0]\n",
    "t = Tree.fromstring(str(sen.constituency)).draw()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OpenHowNet succeeded!\n"
     ]
    }
   ],
   "source": [
    "import OpenHowNet\n",
    "hownet_dict = OpenHowNet.HowNetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "('air', ['sky', 'disseminate', 'gas', 'celestial', 'surround', 'inhale', 'human', 'publish', 'Bearing', 'clean', 'reveal', 'AnimalHuman', 'spotless', 'Demeanor', 'contain', 'Countenance', 'weather'])\n",
      "('has', ['FuncWord'])\n",
      "('higher', ['education', 'GreaterThanNormal', 'HighRank', 'more'])\n",
      "('resolution', ['human', 'resolute', 'willing', 'text', 'handle', 'implement', 'display', 'part', 'Will', 'Performance', 'distinguish', 'aspiration', 'fact', 'analyze', 'explain', 'Vachieve', 'result', 'decide'])\n",
      "('but', ['FuncWord'])\n",
      "('the', ['FuncWord'])\n",
      "('fonts', [])\n",
      "('are', ['FuncWord'])\n",
      "('small', ['negligible', 'small', 'StatureShort'])\n",
      "('.', ['punc'])\n"
     ]
    }
   ],
   "source": [
    "tok_sememes = hownet_dict.get_sememes_by_word(word=\"fonts\", display='list', merge=True, expanded_layer=-1, K=None)\n",
    "text_token = text.split()\n",
    "sememes = []\n",
    "for token in text_token:\n",
    "    tok_sememes = hownet_dict.get_sememes_by_word(word=token, display='list', merge=True, expanded_layer=-1, K=None)\n",
    "    tok_sememes = [str(s).split(\"|\")[0] for s in tok_sememes]\n",
    "    sememes.append((token, tok_sememes))\n",
    "print(len(sememes))\n",
    "for s in sememes:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\semeval14\\laptop_test.raw\n",
      "{1: 292, 2: 91, 3: 31, 4: 10, 5: 5, 6: 1}\n",
      "C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\semeval14\\laptop_train.raw\n",
      "{2: 319, 3: 125, 4: 45, 1: 1013, 6: 6, 5: 9, 13: 1, 7: 4}\n"
     ]
    }
   ],
   "source": [
    "dataset = [\"semeval14\\\\laptop_test.raw\", \"semeval14\\\\laptop_train.raw\"]\n",
    "def count_sentence(file):\n",
    "    fin = open(file, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    lines = fin.readlines()\n",
    "    dic = {}\n",
    "    for i in range(0, len(lines), 3):\n",
    "        text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "        aspect = lines[i + 1].lower().strip()\n",
    "        original_line = text_left + \" \" + aspect + \" \" + text_right\n",
    "        value = dic.get(original_line, 0)\n",
    "        dic[original_line] = value+1\n",
    "    count = {}\n",
    "    for k, v in dic.items():\n",
    "        value = count.get(v, 0)\n",
    "        count[v] = value+1\n",
    "    import json\n",
    "    dic_d = json.dumps(dic, sort_keys=False, indent=4, separators=(',', ': '))\n",
    "    save_path = file +\".json\"\n",
    "    f = open(save_path, 'w')\n",
    "    f.write(dic_d)\n",
    "    print(count)\n",
    "root = r\"C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\"\n",
    "for file in dataset:\n",
    "    path = root + \"\\\\\" + file\n",
    "    print(path)\n",
    "    count_sentence(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\T_data\\test.raw\n",
      "{1: 692}\n",
      "{'-1\\r\\n': 173, '1\\r\\n': 173, '0\\r\\n': 346}\n",
      "C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\T_data\\train.raw\n",
      "{1: 6236, 2: 6}\n",
      "{'-1\\r\\n': 1560, '1\\r\\n': 1561, '0\\r\\n': 3127}\n"
     ]
    }
   ],
   "source": [
    "dataset = [\"T_data\\\\test.raw\", \"T_data\\\\train.raw\"]\n",
    "def count_sentence(file):\n",
    "    fin = open(file, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    lines = fin.readlines()\n",
    "    dic = {}\n",
    "    label = {}\n",
    "    for i in range(0, len(lines), 3):\n",
    "        text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "        aspect = lines[i + 1].lower().strip()\n",
    "        original_line = text_left + \" \" + aspect + \" \" + text_right\n",
    "        value = dic.get(original_line, 0)\n",
    "        dic[original_line] = value+1\n",
    "\n",
    "        label_n = label.get(lines[i+2], 0)\n",
    "        label[lines[i+2]] = label_n+1\n",
    "    count = {}\n",
    "    for k, v in dic.items():\n",
    "        value = count.get(v, 0)\n",
    "        count[v] = value+1\n",
    "        \n",
    "    print(count)\n",
    "    print(label)\n",
    "root = r\"C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\"\n",
    "for file in dataset:\n",
    "    path = root + \"\\\\\" + file\n",
    "    print(path)\n",
    "    count_sentence(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\mams\\mams_test.raw\n",
      "{3: 168, 2: 321, 6: 5, 4: 38, 5: 9, 10: 2}\n",
      "C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\mams\\mams_train.raw\n",
      "{3: 1139, 2: 3129, 4: 311, 5: 109, 7: 11, 6: 40, 8: 5, 9: 2, 10: 1}\n",
      "C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\mams\\mams_val.raw\n",
      "{3: 130, 2: 353, 4: 51, 5: 15, 6: 4, 9: 1, 7: 1}\n"
     ]
    }
   ],
   "source": [
    "dataset = [\"mams\\mams_test.raw\", \"mams\\mams_train.raw\",\"mams\\mams_val.raw\"]\n",
    "def count_sentence_mams(file):\n",
    "    fin = open(file, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    lines = fin.readlines()\n",
    "    dic = {}\n",
    "    for i in range(0, len(lines), 3):\n",
    "        text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "        aspect = lines[i + 1].lower().strip()\n",
    "        original_line = text_left + \" \" + aspect + \" \" + text_right\n",
    "        value = dic.get(original_line, 0)\n",
    "        dic[original_line] = value+1\n",
    "    dic_n = {}\n",
    "    for k, v in dic.items():\n",
    "        if v == 1:\n",
    "            dic_n[k] = 2\n",
    "        else:\n",
    "            dic_n[k] = v\n",
    "    count = {}\n",
    "    for k, v in dic_n.items():\n",
    "        value = count.get(v, 0)\n",
    "        count[v] = value+1\n",
    "    print(count)\n",
    "    import json\n",
    "    dic_d = json.dumps(dic_n, sort_keys=False, indent=4, separators=(',', ': '))\n",
    "    save_path = file +\".json\"\n",
    "    f = open(save_path, 'w')\n",
    "    f.write(dic_d)\n",
    "        \n",
    "root = r\"C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\"\n",
    "for file in dataset:\n",
    "    path = root + \"\\\\\" + file\n",
    "    print(path)\n",
    "    count_sentence_mams(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\semeval14\\restaurant_test.raw\n",
      "{1: 332, 2: 182, 3: 78, 5: 11, 4: 26, 6: 2, 12: 1, 7: 1}\n",
      "C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\semeval14\\restaurant_train.raw\n",
      "{1: 1148, 3: 249, 4: 99, 7: 4, 2: 521, 5: 30, 6: 12, 8: 2, 9: 1}\n"
     ]
    }
   ],
   "source": [
    "dataset = [\"semeval14\\\\restaurant_test.raw\",\"semeval14\\\\restaurant_train.raw\"]\n",
    "def count_sentence(file):\n",
    "    fin = open(file, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    lines = fin.readlines()\n",
    "    dic = {}\n",
    "    for i in range(0, len(lines), 3):\n",
    "        text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "        aspect = lines[i + 1].lower().strip()\n",
    "        original_line = text_left + \" \" + aspect + \" \" + text_right\n",
    "        value = dic.get(original_line, 0)\n",
    "        dic[original_line] = value+1\n",
    "    count = {}\n",
    "    for k, v in dic.items():\n",
    "        value = count.get(v, 0)\n",
    "        count[v] = value+1\n",
    "        \n",
    "    print(count)\n",
    "    import json\n",
    "    dic_d = json.dumps(dic, sort_keys=False, indent=4, separators=(',', ': '))\n",
    "    save_path = file +\".json\"\n",
    "    f = open(save_path, 'w')\n",
    "    f.write(dic_d)\n",
    "root = r\"C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\"\n",
    "for file in dataset:\n",
    "    path = root + \"\\\\\" + file\n",
    "    print(path)\n",
    "    count_sentence(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "469748068ad3701aa1fc60d7c12d0f6ede39b71e62e3e71246d2bd81015eaa01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
