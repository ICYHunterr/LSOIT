{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OpenHowNet succeeded!\n"
     ]
    }
   ],
   "source": [
    "import OpenHowNet\n",
    "hownet_dict = OpenHowNet.HowNetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\T_data\\train.raw\n",
      "C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\T_data\\test.raw\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dataset = [\"mams_small\\mams_test.raw\", \"mams_small\\mams_train.raw\", \"mams_small\\mams_val.raw\",\n",
    "\"mams\\mams_test.raw\", \"mams\\mams_train.raw\",\"mams\\mams_val.raw\",\"semeval14\\\\restaurant_test.raw\",\n",
    "\"semeval14\\\\restaurant_train.raw\",\"semeval15\\\\restaurant_train.raw\", \"semeval15\\\\restaurant_test.raw\",\n",
    "            \"semeval16\\\\restaurant_test.raw\", \"semeval16\\\\restaurant_train.raw\",\"Z_data\\\\train.raw\", \"Z_data\\\\test.raw\", \"Z_data\\\\dev.raw\"]\n",
    "dataset2 = [\"semeval14\\\\laptop_test.raw\", \"semeval14\\\\laptop_train.raw\"]\n",
    "dataset3 = [\"T_data\\\\train.raw\", \"T_data\\\\test.raw\"]\n",
    "def get_sememe(filename):\n",
    "    fin = open(filename, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    lines = fin.readlines()\n",
    "    all_sememe = []\n",
    "    for i in range(0, len(lines), 3):\n",
    "        left_and_right = lines[i].strip().split(\"$T$\")\n",
    "        sentence = left_and_right[0] +lines[i+1].strip() + left_and_right[1]\n",
    "        sememe = []\n",
    "        token = sentence.split()\n",
    "        for tok in token:\n",
    "            tok = tok.lower()\n",
    "            tok_sememes = hownet_dict.get_sememes_by_word(word=tok, display='list', merge=True, expanded_layer=-1, K=None)\n",
    "            tok_sememes = [str(s).split(\"|\")[0] for s in tok_sememes]\n",
    "            sememe.append(tok_sememes)\n",
    "        all_sememe.append(sememe)\n",
    "\n",
    "    sememe_data = np.array(all_sememe, dtype=\"object\")\n",
    "    np.save(filename+\".sememe.npy\", sememe_data)\n",
    "root = r\"C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\"\n",
    "for file in dataset3:\n",
    "    path = root + \"\\\\\" + file\n",
    "    print(path)\n",
    "    get_sememe(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 20:35:59 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-01-10 20:35:59 INFO: Use device: gpu\n",
      "2023-01-10 20:35:59 INFO: Loading: tokenize\n",
      "2023-01-10 20:35:59 INFO: Loading: pos\n",
      "2023-01-10 20:35:59 INFO: Loading: lemma\n",
      "2023-01-10 20:35:59 INFO: Loading: depparse\n",
      "2023-01-10 20:36:00 INFO: Loading: sentiment\n",
      "2023-01-10 20:36:01 INFO: Loading: constituency\n",
      "2023-01-10 20:36:01 INFO: Loading: ner\n",
      "2023-01-10 20:36:02 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from nltk import Tree\n",
    "import stanza\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "datasets = [\"mams_small\\mams_test.raw\", \"mams_small\\mams_train.raw\", \"mams_small\\mams_val.raw\",\n",
    "\"mams\\mams_test.raw\", \"mams\\mams_train.raw\",\"mams\\mams_val.raw\",\"semeval14\\\\restaurant_test.raw\",\n",
    "\"semeval14\\\\restaurant_train.raw\",\"semeval15\\\\restaurant_train.raw\", \"semeval15\\\\restaurant_test.raw\",\n",
    "            \"semeval16\\\\restaurant_test.raw\", \"semeval16\\\\restaurant_train.raw\",]\n",
    "datasets2 = [\n",
    "            \"Z_data\\\\train.raw\", \"Z_data\\\\test.raw\", \"Z_data\\\\dev.raw\"]\n",
    "dataset3 = [\"T_data\\\\train.raw\", \"T_data\\\\test.raw\"]\n",
    "fname = r\"C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\\semeval14\\laptop_test.raw\"\n",
    "def get_const(filename):\n",
    "    fin = open(filename, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    lines = fin.readlines()\n",
    "    all_tag_path = []\n",
    "    for i in range(0, len(lines), 3):\n",
    "        left_and_right = lines[i].strip().split(\"$T$\")\n",
    "        sentence = left_and_right[0] +lines[i+1].strip() + left_and_right[1]\n",
    "        sentence = sentence.replace(\"(\", \"（\")\n",
    "        sentence = sentence.replace(\")\", \"）\")\n",
    "        try:\n",
    "            doc = nlp(sentence)\n",
    "            sen = doc.sentences[0]\n",
    "            t = Tree.fromstring(str(sen.constituency))  \n",
    "            leaves_pos = t.treepositions('leaves')\n",
    "            sentence_tag_path = []\n",
    "            for leaves in leaves_pos:\n",
    "                word_tag_path = [t[leaves]]\n",
    "                for i in range(1, len(leaves)):\n",
    "                    word_tag_path.append(t[leaves[:-i]].label())\n",
    "                word_tag_path.reverse()\n",
    "                sentence_tag_path.append(word_tag_path)\n",
    "\n",
    "            text = sentence.split()\n",
    "            # 句法切词和空格切词可能不一致\n",
    "            path_dic = {}\n",
    "            flag_dic = {}\n",
    "            flag_count_dic = {}\n",
    "            flag_count_use_dic = {}\n",
    "            # 句子原本词的个数\n",
    "            \n",
    "            for w in text:\n",
    "                value = flag_dic.get(w, \"None\")\n",
    "                if value == \"None\":\n",
    "                    flag_dic[w] = 1\n",
    "                    flag_count_dic[w] = 1\n",
    "                    flag_count_use_dic[w] = 1\n",
    "                else:\n",
    "                    flag_dic[w] += 1\n",
    "            for path in sentence_tag_path:\n",
    "                value = path_dic.get(path[-1], \"None\")\n",
    "                value_n = flag_dic.get(path[-1], 0)\n",
    "                if value_n == 1:\n",
    "                    path_dic[path[-1]] = path\n",
    "                elif value_n > 1:\n",
    "                    path_dic[path[-1]+str(flag_count_dic[path[-1]])] = path\n",
    "                    flag_count_dic[path[-1]] += 1\n",
    "\n",
    "            complete_path = []\n",
    "            for w in text:\n",
    "                if flag_dic[w] == 1:\n",
    "                    complete_path.append(path_dic.get(w, []))\n",
    "                else:\n",
    "                    complete_path.append(path_dic.get(w+str(flag_count_use_dic[w]), []))\n",
    "                    flag_count_use_dic[w] += 1\n",
    "            all_tag_path.append(complete_path)\n",
    "        except:\n",
    "            text = sentence.split()\n",
    "            all_tag_path.append([[0] for i in range(len(text))])\n",
    "\n",
    "    data = np.array(all_tag_path, dtype=object)\n",
    "    cons_path = filename + \".const.npy\"\n",
    "    # print(all_tag_path)\n",
    "    np.save(cons_path, data)\n",
    "root = r\"C:\\Users\\ASUS\\OneDrive\\桌面\\indo-dotGCN\\datasets\"\n",
    "for dataset in dataset3:\n",
    "    path = root + \"\\\\\" + dataset\n",
    "    get_const(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "469748068ad3701aa1fc60d7c12d0f6ede39b71e62e3e71246d2bd81015eaa01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
